import numpy as np import pandas as pd from scipy.signal import savgol_filter import os from gymnasium import Env, spaces from stable_baselines3 import PPO from stable_baselines3.common.env_checker import check_env from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

---------------------- Step 1: Generate and Save Simulated Data ----------------------

np.random.seed(42) t = np.arange(0, 20000) BASE_INDEX = 23000

trend = 1 + 0.000005 * t amplitude_growth = 1 + 0.00005 * t frequency_modulation = t / (30 + 0.0005 * t) wave = np.sin(frequency_modulation) * amplitude_growth noise = np.random.normal(0, 0.02, len(t)) price_series = (wave + trend + noise) * 100 + BASE_INDEX

os.makedirs("data", exist_ok=True) pd.DataFrame({"current": price_series}).to_csv("data/hangseng_simulated.csv", index=False)

---------------------- Step 2: Define Custom Environment ----------------------

class HangSengSellEnv(Env): def init(self, price_series, seq_len=30, max_inventory=100, max_steps=300): super().init() self.price_series = price_series self.seq_len = seq_len self.max_inventory = max_inventory self.max_steps = max_steps

self.action_space = spaces.Discrete(11)
    self.observation_space = spaces.Box(low=0, high=1, shape=(seq_len + 4,), dtype=np.float32)
    self.reset()

def reset(self, seed=None, options=None):
    super().reset(seed=seed)
    self.pointer = np.random.randint(0, len(self.price_series) - self.max_steps - self.seq_len - 300)
    self.current_step = 0
    self.inventory = self.max_inventory
    self.total_cash = 0.0
    self.done = False
    return self._get_obs(), {}

def _get_obs(self):
    window = self.price_series[self.pointer + self.current_step:
                               self.pointer + self.current_step + self.seq_len]
    denoised = savgol_filter(window, window_length=7, polyorder=2)
    normed = (denoised - np.min(denoised)) / (np.max(denoised) - np.min(denoised) + 1e-6)
    slope = np.polyfit(np.arange(len(denoised)), denoised, 1)[0]
    volatility = np.std(denoised)
    obs = np.concatenate([
        normed,
        [slope / 10],
        [volatility / 100],
        [self.inventory / self.max_inventory],
        [self.current_step / self.max_steps]
    ])
    return obs.astype(np.float32)

def step(self, action):
    units_to_sell = min(action, self.inventory)
    current_idx = self.pointer + self.current_step + self.seq_len - 1
    current_price = self.price_series[current_idx]

    future_window = self.price_series[current_idx + 1 : current_idx + 300]
    future_max = np.max(future_window) if len(future_window) > 0 else current_price

    price_gap = future_max - current_price
    proximity_bonus = np.exp(-price_gap / 20)

    reward = units_to_sell * (current_price / BASE_INDEX) * proximity_bonus

    if self.inventory > 0 and self.current_step == self.max_steps - 1:
        reward += self.inventory * (current_price / BASE_INDEX) * 0.2

    self.inventory -= units_to_sell
    self.total_cash += reward
    self.current_step += 1
    self.done = self.inventory <= 0 or self.current_step >= self.max_steps

    return self._get_obs(), reward, self.done, False, {}

---------------------- Step 3: Create Vectorized Environment ----------------------

def exp_decay_schedule(initial_lr=0.0003, decay_rate=0.95): def schedule(progress_remaining): step = 1.0 - progress_remaining return initial_lr * (decay_rate ** (step * 10)) return schedule

price_series = pd.read_csv("data/hangseng_simulated.csv")["current"].values base_env = lambda: HangSengSellEnv(price_series=price_series) vec_env = DummyVecEnv([base_env]) vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)

---------------------- Step 4: Define PPO Model with Enhancements ----------------------

policy_kwargs = dict( net_arch=[dict(pi=[64, 64], vf=[256, 128, 64])] )

model = PPO( "MlpPolicy", vec_env, verbose=1, learning_rate=exp_decay_schedule(0.0003, 0.95), n_steps=2048, batch_size=64, gae_lambda=0.92, vf_coef=0.8, policy_kwargs=policy_kwargs )

---------------------- Step 5: Train the Agent ----------------------

model.learn(total_timesteps=200_000) vec_env.save("data/vec_normalize.pkl")

---------------------- Step 6: Save Model ----------------------

os.makedirs("trained_model", exist_ok=True) model.save("trained_model/ppo_hangseng_agent") print("âœ… Model and environment normalization saved.")

